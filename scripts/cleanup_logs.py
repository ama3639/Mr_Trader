#!/usr/bin/env python3
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ MrTrader Bot
"""
import os
import sys
import gzip
import shutil
from pathlib import Path
from datetime import datetime, timedelta
import argparse

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÙˆØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.logger import logger


class LogCleaner:
    """Ú©Ù„Ø§Ø³ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.logs_dir = project_root / "logs"
        self.archive_dir = self.logs_dir / "archive"
        self.archive_dir.mkdir(exist_ok=True)
    
    def cleanup_logs(self, days_to_keep=30, compress_old=True, delete_old=False):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        try:
            if not self.logs_dir.exists():
                print("ğŸ“ Ù¾ÙˆØ´Ù‡ logs ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return
            
            print(f"ğŸ§¹ Ø´Ø±ÙˆØ¹ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ (Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ {days_to_keep} Ø±ÙˆØ²)")
            
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯
            log_files = list(self.logs_dir.glob("*.log*"))
            
            if not log_files:
                print("ğŸ“„ ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return
            
            # Ø¢Ù…Ø§Ø±
            compressed_count = 0
            deleted_count = 0
            total_size_freed = 0
            
            for log_file in log_files:
                try:
                    # Ø¨Ø±Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® ÙØ§ÛŒÙ„
                    file_date = datetime.fromtimestamp(log_file.stat().st_mtime)
                    
                    if file_date < cutoff_date:
                        file_size = log_file.stat().st_size
                        
                        if delete_old:
                            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ
                            log_file.unlink()
                            deleted_count += 1
                            total_size_freed += file_size
                            print(f"ğŸ—‘ï¸ Ø­Ø°Ù Ø´Ø¯: {log_file.name}")
                            
                        elif compress_old and not log_file.name.endswith('.gz'):
                            # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒ
                            compressed_file = self.archive_dir / f"{log_file.name}.gz"
                            
                            with open(log_file, 'rb') as f_in:
                                with gzip.open(compressed_file, 'wb') as f_out:
                                    shutil.copyfileobj(f_in, f_out)
                            
                            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ
                            log_file.unlink()
                            
                            compressed_count += 1
                            compression_ratio = compressed_file.stat().st_size / file_size
                            print(f"ğŸ“¦ ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯: {log_file.name} (Ù†Ø³Ø¨Øª: {compression_ratio:.2f})")
                    
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {log_file.name}: {e}")
                    continue
            
            # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
            print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ:")
            if compress_old:
                print(f"ğŸ“¦ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯Ù‡: {compressed_count}")
            if delete_old:
                print(f"ğŸ—‘ï¸ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡: {deleted_count}")
                print(f"ğŸ’¾ ÙØ¶Ø§ÛŒ Ø¢Ø²Ø§Ø¯ Ø´Ø¯Ù‡: {self._format_size(total_size_freed)}")
            
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¢Ø±Ø´ÛŒÙˆ Ù‚Ø¯ÛŒÙ…ÛŒ
            self._cleanup_archive(days_to_keep * 2)  # Ø¢Ø±Ø´ÛŒÙˆ 2 Ø¨Ø±Ø§Ø¨Ø± Ø¨ÛŒØ´ØªØ± Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø´ÙˆØ¯
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§: {e}")
            logger.error(f"Error cleaning logs: {e}")
    
    def _cleanup_archive(self, archive_days=60):
        """Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¢Ø±Ø´ÛŒÙˆ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        try:
            cutoff_date = datetime.now() - timedelta(days=archive_days)
            archive_files = list(self.archive_dir.glob("*.gz"))
            
            deleted_archive_count = 0
            
            for archive_file in archive_files:
                file_date = datetime.fromtimestamp(archive_file.stat().st_mtime)
                
                if file_date < cutoff_date:
                    archive_file.unlink()
                    deleted_archive_count += 1
                    print(f"ğŸ—‘ï¸ Ø¢Ø±Ø´ÛŒÙˆ Ø­Ø°Ù Ø´Ø¯: {archive_file.name}")
            
            if deleted_archive_count > 0:
                print(f"ğŸ“ {deleted_archive_count} ÙØ§ÛŒÙ„ Ø¢Ø±Ø´ÛŒÙˆ Ù‚Ø¯ÛŒÙ…ÛŒ Ø­Ø°Ù Ø´Ø¯")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¢Ø±Ø´ÛŒÙˆ: {e}")
    
    def analyze_logs(self):
        """ØªØ­Ù„ÛŒÙ„ Ù„Ø§Ú¯â€ŒÙ‡Ø§"""
        try:
            if not self.logs_dir.exists():
                print("ğŸ“ Ù¾ÙˆØ´Ù‡ logs ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                return
            
            print("ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ù„Ø§Ú¯â€ŒÙ‡Ø§:")
            
            # Ø¢Ù…Ø§Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯
            log_files = list(self.logs_dir.glob("*.log*"))
            archive_files = list(self.archive_dir.glob("*.gz")) if self.archive_dir.exists() else []
            
            total_size = sum(f.stat().st_size for f in log_files)
            archive_size = sum(f.stat().st_size for f in archive_files) if archive_files else 0
            
            print(f"ğŸ“„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯ ÙØ¹Ø§Ù„: {len(log_files)}")
            print(f"ğŸ“¦ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ø±Ø´ÛŒÙˆ: {len(archive_files)}")
            print(f"ğŸ’¾ Ø­Ø¬Ù… Ú©Ù„ Ù„Ø§Ú¯â€ŒÙ‡Ø§: {self._format_size(total_size)}")
            print(f"ğŸ’¾ Ø­Ø¬Ù… Ø¢Ø±Ø´ÛŒÙˆ: {self._format_size(archive_size)}")
            
            # ØªØ­Ù„ÛŒÙ„ Ø³Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
            if log_files:
                oldest_file = min(log_files, key=lambda f: f.stat().st_mtime)
                newest_file = max(log_files, key=lambda f: f.stat().st_mtime)
                
                oldest_date = datetime.fromtimestamp(oldest_file.stat().st_mtime)
                newest_date = datetime.fromtimestamp(newest_file.stat().st_mtime)
                
                print(f"ğŸ“… Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ù„Ø§Ú¯: {oldest_date.strftime('%Y-%m-%d %H:%M')} ({oldest_file.name})")
                print(f"ğŸ“… Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù„Ø§Ú¯: {newest_date.strftime('%Y-%m-%d %H:%M')} ({newest_file.name})")
                
                age_days = (datetime.now() - oldest_date).days
                print(f"ğŸ“Š Ø¯Ø§Ù…Ù†Ù‡ Ø³Ù†ÛŒ: {age_days} Ø±ÙˆØ²")
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯
            large_files = [f for f in log_files if f.stat().st_size > 10 * 1024 * 1024]  # Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² 10MB
            if large_files:
                print(f"\nğŸ“ˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ (>10MB):")
                for file in large_files:
                    size = self._format_size(file.stat().st_size)
                    print(f"  â€¢ {file.name}: {size}")
            
            # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
            self._suggest_cleanup_strategy(log_files, total_size)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù„Ø§Ú¯â€ŒÙ‡Ø§: {e}")
    
    def _suggest_cleanup_strategy(self, log_files, total_size):
        """Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ"""
        print(f"\nğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:")
        
        if total_size > 100 * 1024 * 1024:  # Ø¨ÛŒØ´ Ø§Ø² 100MB
            print("  â€¢ Ø­Ø¬Ù… Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø²ÛŒØ§Ø¯ Ø§Ø³ØªØŒ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        if len(log_files) > 50:
            print("  â€¢ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯ Ø²ÛŒØ§Ø¯ Ø§Ø³Øª")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
        old_files = []
        cutoff_date = datetime.now() - timedelta(days=30)
        
        for file in log_files:
            file_date = datetime.fromtimestamp(file.stat().st_mtime)
            if file_date < cutoff_date:
                old_files.append(file)
        
        if old_files:
            old_size = sum(f.stat().st_size for f in old_files)
            print(f"  â€¢ {len(old_files)} ÙØ§ÛŒÙ„ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ø§Ø² 30 Ø±ÙˆØ² ({self._format_size(old_size)})")
            print("  â€¢ Ø¯Ø³ØªÙˆØ± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ: python scripts/cleanup_logs.py --days 30 --compress")
        else:
            print("  â€¢ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ø®ÙˆØ¨ÛŒ Ù‡Ø³ØªÙ†Ø¯")
    
    def _format_size(self, size_bytes):
        """ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„"""
        if size_bytes >= 1024**3:
            return f"{size_bytes / (1024**3):.2f} GB"
        elif size_bytes >= 1024**2:
            return f"{size_bytes / (1024**2):.2f} MB"
        elif size_bytes >= 1024:
            return f"{size_bytes / 1024:.2f} KB"
        else:
            return f"{size_bytes} bytes"
    
    def rotate_current_logs(self):
        """Ú†Ø±Ø®Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ"""
        try:
            current_logs = [
                "mrtrader.log",
                "errors.log",
                "api.log",
                "user_activity.log"
            ]
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            rotated_count = 0
            
            for log_name in current_logs:
                log_file = self.logs_dir / log_name
                
                if log_file.exists() and log_file.stat().st_size > 0:
                    # ØªØ¹ÛŒÛŒÙ† Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
                    name_parts = log_name.split('.')
                    rotated_name = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
                    rotated_file = self.logs_dir / rotated_name
                    
                    # Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ ÙØ§ÛŒÙ„
                    log_file.rename(rotated_file)
                    rotated_count += 1
                    
                    print(f"ğŸ”„ Ú†Ø±Ø®Ø´: {log_name} â†’ {rotated_name}")
            
            print(f"âœ… {rotated_count} ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ Ú†Ø±Ø®Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú†Ø±Ø®Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§: {e}")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    parser = argparse.ArgumentParser(description="Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ MrTrader Bot")
    parser.add_argument("--days", type=int, default=30,
                       help="ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 30)")
    parser.add_argument("--compress", action="store_true",
                       help="ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ")
    parser.add_argument("--delete", action="store_true",
                       help="Ø­Ø°Ù ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (Ø¨Ø¬Ø§ÛŒ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ)")
    parser.add_argument("--analyze", action="store_true",
                       help="ÙÙ‚Ø· ØªØ­Ù„ÛŒÙ„ Ù„Ø§Ú¯â€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)")
    parser.add_argument("--rotate", action="store_true",
                       help="Ú†Ø±Ø®Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ")
    
    args = parser.parse_args()
    
    try:
        cleaner = LogCleaner()
        
        if args.analyze:
            cleaner.analyze_logs()
        elif args.rotate:
            cleaner.rotate_current_logs()
        else:
            cleaner.cleanup_logs(
                days_to_keep=args.days,
                compress_old=args.compress,
                delete_old=args.delete
            )
        
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)